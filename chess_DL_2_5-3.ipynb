{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
    "import torch.nn.functional as F\n",
    "import chess\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NUMBER = 2\n",
    "MODEL_VERSION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network model for evaluating chess positions.\n",
    "\n",
    "    This model takes a chess position as input and predicts the evaluation score\n",
    "    for that position. It consists of convolutional and fully connected layers.\n",
    "\n",
    "    Attributes:\n",
    "        conv1 (nn.Conv2d): First convolutional layer.\n",
    "        conv2 (nn.Conv2d): Second convolutional layer.\n",
    "        fc1 (nn.Linear): First fully connected layer.\n",
    "        fc2 (nn.Linear): Second fully connected layer.\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Performs forward pass through the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the EvalNet class\n",
    "\n",
    "        Args:\n",
    "        - None\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        super(EvalNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(12, 16, kernel_size = 5, stride = 1, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(16, 24, kernel_size = 3, stride = 1, padding = 1) \n",
    "        self.fc1 = nn.Linear(24 * 6 * 6, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "        - x (torch.Tensor): Input tensor of shape (batch_size, 12, 8, 8)\n",
    "        \n",
    "        Returns:\n",
    "        - torch.Tensor: Output tensor of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        return self.fc2(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fen_str_to_3d_tensor(fen: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts a FEN string representation of a chess position to a 3D tensor.\n",
    "\n",
    "    Args:\n",
    "    - fen (str): The FEN string representing the chess position.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: A 3D tensor representing the chess position, where each element\n",
    "                    corresponds to a piece on the board.\n",
    "\n",
    "    Example:\n",
    "        fen = 'rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1'\n",
    "        tensor = fen_str_to_3d_tensor(fen)\n",
    "    \"\"\"\n",
    "    piece_to_int = {\n",
    "        'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
    "        'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11,\n",
    "    }\n",
    "\n",
    "    board = np.zeros((12, 8, 8), dtype=np.float32)\n",
    "    \n",
    "    # Split the FEN string into parts ## 'rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1'\n",
    "    fen_parts = fen.split(' ')\n",
    "    fen_rows = fen_parts[0].split('/') # Only process the board position (the first part)\n",
    "    \n",
    "    for row_idx, row in enumerate(fen_rows):\n",
    "        col_idx = 0\n",
    "        for char in row:\n",
    "            if char.isdigit():\n",
    "                col_idx += int(char)\n",
    "            else:\n",
    "                piece = piece_to_int[char]\n",
    "                board[piece, row_idx, col_idx] = 1\n",
    "                col_idx += 1\n",
    "    \n",
    "    return torch.tensor(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Data/DataTrain\"\n",
    "\n",
    "letters_in = 'bcdefghijklmnopqrstuvwxyz'\n",
    "letters_out = 'a'\n",
    "\n",
    "csv_files_train = []\n",
    "csv_files_val = []\n",
    "\n",
    "for let in letters_in:\n",
    "    csv_files_train.extend(glob.glob(f'{path}/Chess_Jan_{let}*'))\n",
    "    csv_files_train.extend(glob.glob(f'{path}/Chess_Feb_{let}*'))\n",
    "    csv_files_train.extend(glob.glob(f'{path}/Chess_Mar_{let}*')) # include in version 1-3\n",
    "    # csv_files_train.extend(glob.glob(f'{path}/Chess_Apr_{let}*'))\n",
    "\n",
    "for let_ in letters_out:\n",
    "    csv_files_val.extend(glob.glob(f'{path}/Chess_Jan_{let_}*'))\n",
    "    csv_files_val.extend(glob.glob(f'{path}/Chess_Feb_{let_}*'))\n",
    "    csv_files_val.extend(glob.glob(f'{path}/Chess_Mar_{let_}*')) # include in version 1-3\n",
    "    # csv_files_val.extend(glob.glob(f'{path}/Chess_Apr_{let_}*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inputs(DF: pl.DataFrame) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Proprocesses the input tensor into batches by applying the fen_str_to_3d_tensor function.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_rows = DF.shape[0]\n",
    "\n",
    "    inputs = torch.zeros(n_rows, 12, 8, 8)\n",
    "\n",
    "    for i in range(n_rows):\n",
    "        inputs[i] = fen_str_to_3d_tensor(DF['board'][i])\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data(csv_file):\n",
    "    \n",
    "    df = pl.read_csv(csv_file, null_values=[\"None\", \"null\", \"#0\", \"#-0\"])\n",
    "    df = df.drop_nulls()\n",
    "\n",
    "    df = df.with_columns(clip=pl.col(\"cp\").clip(-10, 10))\n",
    "\n",
    "    inputs = preprocess_inputs(df)\n",
    "    targets = torch.tensor(df['cp'])\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "\n",
    "class ChessIterableDataset(IterableDataset):\n",
    "    def __init__(self, csv_files, chunksize=50000):\n",
    "        \"\"\"\n",
    "        Initializes the ChessIterableDataset class.\n",
    "\n",
    "        Args:\n",
    "        - csv_files (list): List of CSV file paths.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        self.csv_files = csv_files\n",
    "        self.chunksize = chunksize\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Implements the length method.\n",
    "\n",
    "        Returns:\n",
    "        - int: Length of the dataset.\n",
    "        \"\"\"\n",
    "        return sum(1 for _ in self.__iter__())\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Implements the iterator logic.\n",
    "\n",
    "        Returns:\n",
    "        - Iterator object\n",
    "        \"\"\"\n",
    "        return iter(self.csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_loader, val_data_loader, criterion, optimizer, num_epochs):\n",
    "    \"\"\"\n",
    "    Trains model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        model to be trained.\n",
    "    train_data_loader : torch.utils.data.DataLoader\n",
    "        training data.\n",
    "    val_data_loader : torch.utils.data.DataLoader\n",
    "        validation data.\n",
    "    criterion : torch.nn.modules.loss._Loss\n",
    "        loss function\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        optimizer\n",
    "    num_epochs : int\n",
    "        Number of epochs\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        average training loss for each epoch\n",
    "    list\n",
    "        average validation loss for each epoch\n",
    "\n",
    "    \"\"\"\n",
    "    print(f'Begin Training! (on {device})')\n",
    "\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "\n",
    "    try:\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "            train_running_loss = 0.0\n",
    "            val_running_loss = 0.0\n",
    "\n",
    "            ## TRAINING PHASE =================================\n",
    "            model.train()  # Set the model to training mode\n",
    "\n",
    "            for i, csv_file in enumerate(csv_files_train):\n",
    "\n",
    "                inputs, targets = Data(csv_file)\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device).unsqueeze(1)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                train_outputs = model(inputs)\n",
    "                train_batch_loss = criterion(train_outputs, targets)\n",
    "\n",
    "                print(f\"\\t Training Batch Loss: {train_batch_loss}\")\n",
    "\n",
    "                train_batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_running_loss += train_batch_loss.item()\n",
    "            \n",
    "            ## VALIDATION PHASE =================================\n",
    "            model.eval()  # Set the model to evaluation mode\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                for i, csv_file in enumerate(csv_files_val):\n",
    "\n",
    "                    inputs_val, targets_val = Data(csv_file)\n",
    "\n",
    "                    inputs_val = inputs_val.to(device)\n",
    "                    targets_val = targets_val.to(device).unsqueeze(1)\n",
    "                    \n",
    "                    val_outputs = model(inputs_val) # forward\n",
    "                    val_batch_loss = criterion(val_outputs, targets_val)\n",
    "\n",
    "                    print(f\"\\t Validation Batch Loss: {val_batch_loss}\")\n",
    "\n",
    "                    val_running_loss += val_batch_loss.item()\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_running_loss/len(csv_files_train):.5f}, Validation Loss: {val_running_loss/len(csv_files_val):.5f}')\n",
    "            training_loss_history.append(train_running_loss/len(train_data_loader))\n",
    "            validation_loss_history.append(val_running_loss/len(val_data_loader))\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Manual Stop: Finished Training Early!\")\n",
    "    finally:\n",
    "        torch.save(model, f'models_autosave/autosave{MODEL_NUMBER}-{MODEL_VERSION}.pth')\n",
    "\n",
    "    print(f'Finished Training!')\n",
    "\n",
    "    return training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Training! (on cuda)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training Batch Loss: 2.701425552368164\n",
      "\t Training Batch Loss: 2.844952344894409\n",
      "\t Training Batch Loss: 2.478977918624878\n",
      "\t Training Batch Loss: 2.7950637340545654\n",
      "\t Training Batch Loss: 2.5609829425811768\n",
      "\t Training Batch Loss: 2.7199442386627197\n",
      "\t Training Batch Loss: 2.4520983695983887\n",
      "\t Training Batch Loss: 2.622284412384033\n",
      "\t Training Batch Loss: 2.526482582092285\n",
      "\t Training Batch Loss: 2.6842849254608154\n",
      "\t Training Batch Loss: 2.4676952362060547\n",
      "\t Training Batch Loss: 2.532247304916382\n",
      "\t Training Batch Loss: 2.674440622329712\n",
      "\t Training Batch Loss: 2.5060136318206787\n",
      "\t Training Batch Loss: 2.4860470294952393\n",
      "\t Training Batch Loss: 2.550229549407959\n",
      "\t Training Batch Loss: 2.5054118633270264\n",
      "\t Training Batch Loss: 2.6299426555633545\n",
      "\t Training Batch Loss: 2.5031967163085938\n",
      "\t Training Batch Loss: 2.537966012954712\n",
      "\t Training Batch Loss: 2.552461624145508\n",
      "\t Training Batch Loss: 2.580718755722046\n",
      "\t Training Batch Loss: 2.3462016582489014\n",
      "\t Training Batch Loss: 2.404313802719116\n",
      "\t Training Batch Loss: 2.4297287464141846\n",
      "\t Training Batch Loss: 2.449368715286255\n",
      "\t Training Batch Loss: 2.4214789867401123\n",
      "\t Training Batch Loss: 2.4375741481781006\n",
      "\t Training Batch Loss: 2.4936928749084473\n",
      "\t Training Batch Loss: 2.5973992347717285\n",
      "\t Training Batch Loss: 2.384657859802246\n",
      "\t Training Batch Loss: 2.485398292541504\n",
      "\t Training Batch Loss: 2.431943893432617\n",
      "\t Training Batch Loss: 2.482480764389038\n",
      "\t Training Batch Loss: 2.4860219955444336\n",
      "\t Training Batch Loss: 2.4655535221099854\n",
      "\t Training Batch Loss: 2.4611728191375732\n",
      "\t Training Batch Loss: 2.443337917327881\n",
      "\t Training Batch Loss: 2.4612858295440674\n",
      "\t Training Batch Loss: 2.491029977798462\n",
      "\t Training Batch Loss: 2.5418195724487305\n",
      "\t Training Batch Loss: 2.5470407009124756\n",
      "\t Training Batch Loss: 2.5228285789489746\n",
      "\t Training Batch Loss: 2.5976884365081787\n",
      "\t Training Batch Loss: 2.5360305309295654\n",
      "\t Training Batch Loss: 2.5513858795166016\n",
      "\t Training Batch Loss: 2.46651291847229\n",
      "\t Training Batch Loss: 2.4897100925445557\n",
      "\t Training Batch Loss: 2.4746670722961426\n",
      "\t Training Batch Loss: 2.439704418182373\n",
      "\t Training Batch Loss: 2.4444286823272705\n",
      "\t Training Batch Loss: 2.476378917694092\n",
      "\t Training Batch Loss: 2.5946097373962402\n",
      "\t Training Batch Loss: 2.6313114166259766\n",
      "\t Training Batch Loss: 2.5901341438293457\n",
      "\t Training Batch Loss: 2.5194268226623535\n",
      "\t Training Batch Loss: 2.5446829795837402\n",
      "\t Training Batch Loss: 2.522921562194824\n",
      "\t Training Batch Loss: 2.5466790199279785\n",
      "\t Training Batch Loss: 2.5174057483673096\n",
      "\t Training Batch Loss: 2.4820094108581543\n",
      "\t Training Batch Loss: 2.528671979904175\n",
      "\t Training Batch Loss: 2.6770596504211426\n",
      "\t Training Batch Loss: 2.9939534664154053\n",
      "\t Training Batch Loss: 3.147996187210083\n",
      "\t Training Batch Loss: 2.9190423488616943\n",
      "\t Training Batch Loss: 3.110663414001465\n",
      "\t Training Batch Loss: 2.9832637310028076\n",
      "\t Training Batch Loss: 3.1189849376678467\n",
      "\t Training Batch Loss: 3.123938798904419\n",
      "\t Training Batch Loss: 2.7904584407806396\n",
      "\t Training Batch Loss: 2.9310832023620605\n",
      "\t Training Batch Loss: 2.7576160430908203\n",
      "\t Training Batch Loss: 3.0578558444976807\n",
      "\t Training Batch Loss: 2.647716999053955\n",
      "\t Training Batch Loss: 3.039518117904663\n",
      "\t Training Batch Loss: 2.9403600692749023\n",
      "\t Training Batch Loss: 2.5171256065368652\n",
      "\t Training Batch Loss: 2.716810703277588\n",
      "\t Training Batch Loss: 2.613118886947632\n",
      "\t Training Batch Loss: 2.6439106464385986\n",
      "\t Training Batch Loss: 2.5279555320739746\n",
      "\t Training Batch Loss: 2.573261260986328\n",
      "\t Training Batch Loss: 2.5246644020080566\n",
      "\t Training Batch Loss: 2.4535579681396484\n",
      "\t Training Batch Loss: 2.4570868015289307\n",
      "\t Training Batch Loss: 2.5300657749176025\n",
      "\t Training Batch Loss: 2.4632115364074707\n",
      "\t Training Batch Loss: 2.517096757888794\n",
      "\t Training Batch Loss: 2.5035064220428467\n",
      "\t Training Batch Loss: 2.5042552947998047\n",
      "\t Training Batch Loss: 2.473595142364502\n",
      "\t Training Batch Loss: 2.3917288780212402\n",
      "\t Training Batch Loss: 2.5406250953674316\n",
      "\t Training Batch Loss: 2.391479730606079\n",
      "\t Training Batch Loss: 2.4453446865081787\n",
      "\t Training Batch Loss: 2.4073147773742676\n",
      "\t Training Batch Loss: 2.3890979290008545\n",
      "\t Training Batch Loss: 2.4641366004943848\n",
      "\t Training Batch Loss: 2.462644100189209\n",
      "\t Training Batch Loss: 2.4758565425872803\n",
      "\t Training Batch Loss: 2.4124557971954346\n",
      "\t Training Batch Loss: 2.4973807334899902\n",
      "\t Training Batch Loss: 2.5210633277893066\n",
      "\t Training Batch Loss: 2.461005687713623\n",
      "\t Training Batch Loss: 2.4474475383758545\n",
      "\t Training Batch Loss: 2.5118234157562256\n",
      "\t Training Batch Loss: 2.4939780235290527\n",
      "\t Training Batch Loss: 2.449944496154785\n",
      "\t Training Batch Loss: 2.446446418762207\n",
      "\t Training Batch Loss: 2.4432218074798584\n",
      "\t Training Batch Loss: 2.5192058086395264\n",
      "\t Training Batch Loss: 2.583559274673462\n",
      "\t Training Batch Loss: 2.588144540786743\n",
      "\t Training Batch Loss: 2.718052387237549\n",
      "\t Training Batch Loss: 2.472433090209961\n",
      "\t Training Batch Loss: 2.6368424892425537\n",
      "\t Training Batch Loss: 2.603855609893799\n",
      "\t Training Batch Loss: 2.5782995223999023\n",
      "\t Training Batch Loss: 2.5235941410064697\n",
      "\t Training Batch Loss: 2.5805561542510986\n",
      "\t Training Batch Loss: 2.5481603145599365\n",
      "\t Training Batch Loss: 2.5431251525878906\n",
      "\t Training Batch Loss: 2.578784227371216\n",
      "\t Training Batch Loss: 2.5003180503845215\n",
      "\t Training Batch Loss: 2.5140559673309326\n",
      "\t Training Batch Loss: 2.5208311080932617\n",
      "\t Training Batch Loss: 2.4846129417419434\n",
      "\t Training Batch Loss: 2.5439939498901367\n",
      "\t Training Batch Loss: 2.5526084899902344\n",
      "\t Training Batch Loss: 2.6269373893737793\n",
      "\t Training Batch Loss: 2.39922833442688\n",
      "\t Training Batch Loss: 2.5653111934661865\n",
      "\t Training Batch Loss: 2.767838478088379\n",
      "\t Training Batch Loss: 3.515131950378418\n",
      "\t Training Batch Loss: 4.180539131164551\n",
      "\t Training Batch Loss: 3.2747397422790527\n",
      "\t Training Batch Loss: 4.058854103088379\n",
      "\t Training Batch Loss: 3.7068467140197754\n",
      "\t Training Batch Loss: 3.773540735244751\n",
      "\t Training Batch Loss: 3.7309281826019287\n",
      "\t Training Batch Loss: 3.539268970489502\n",
      "\t Training Batch Loss: 3.962353229522705\n",
      "\t Training Batch Loss: 3.771880865097046\n",
      "\t Training Batch Loss: 3.5879464149475098\n",
      "\t Training Batch Loss: 3.881706476211548\n",
      "\t Training Batch Loss: 3.634673595428467\n",
      "\t Training Batch Loss: 3.512816905975342\n",
      "\t Training Batch Loss: 3.562004804611206\n",
      "\t Training Batch Loss: 3.523221492767334\n",
      "\t Training Batch Loss: 3.549633026123047\n",
      "\t Training Batch Loss: 3.264756441116333\n",
      "\t Training Batch Loss: 3.2885141372680664\n",
      "\t Training Batch Loss: 3.3391497135162354\n",
      "\t Training Batch Loss: 3.1806347370147705\n",
      "\t Training Batch Loss: 3.1012165546417236\n",
      "\t Training Batch Loss: 3.1019399166107178\n",
      "\t Training Batch Loss: 2.969566583633423\n",
      "\t Training Batch Loss: 2.9307641983032227\n",
      "\t Training Batch Loss: 2.7815613746643066\n",
      "\t Training Batch Loss: 2.925968885421753\n",
      "\t Training Batch Loss: 2.8336379528045654\n",
      "\t Training Batch Loss: 2.852632761001587\n",
      "\t Training Batch Loss: 2.892118215560913\n",
      "\t Training Batch Loss: 2.766268491744995\n",
      "\t Training Batch Loss: 2.8470659255981445\n",
      "\t Training Batch Loss: 2.839151620864868\n",
      "\t Training Batch Loss: 2.7507314682006836\n",
      "\t Training Batch Loss: 2.6875176429748535\n",
      "\t Training Batch Loss: 2.7471530437469482\n",
      "\t Training Batch Loss: 2.607757091522217\n",
      "\t Training Batch Loss: 2.585792303085327\n",
      "\t Training Batch Loss: 2.596048355102539\n",
      "\t Training Batch Loss: 2.6211490631103516\n",
      "\t Training Batch Loss: 2.6647768020629883\n",
      "\t Training Batch Loss: 2.6555581092834473\n",
      "\t Training Batch Loss: 2.5977461338043213\n",
      "\t Training Batch Loss: 2.594974994659424\n",
      "\t Training Batch Loss: 2.5803065299987793\n",
      "\t Training Batch Loss: 2.6525301933288574\n",
      "\t Training Batch Loss: 2.489250421524048\n",
      "\t Training Batch Loss: 2.5948126316070557\n",
      "\t Training Batch Loss: 2.5186195373535156\n",
      "\t Training Batch Loss: 2.543001413345337\n",
      "\t Training Batch Loss: 2.666987419128418\n",
      "\t Training Batch Loss: 2.632605791091919\n",
      "\t Training Batch Loss: 2.5830671787261963\n",
      "\t Training Batch Loss: 2.6068992614746094\n",
      "\t Training Batch Loss: 2.585726499557495\n",
      "\t Training Batch Loss: 2.6567018032073975\n",
      "\t Training Batch Loss: 2.5127146244049072\n",
      "\t Training Batch Loss: 2.5554656982421875\n",
      "\t Training Batch Loss: 2.670365571975708\n",
      "\t Training Batch Loss: 2.641967296600342\n",
      "\t Training Batch Loss: 2.6820688247680664\n",
      "\t Training Batch Loss: 2.6242332458496094\n",
      "\t Training Batch Loss: 2.4863457679748535\n",
      "\t Training Batch Loss: 2.6531853675842285\n",
      "\t Training Batch Loss: 2.615189790725708\n",
      "\t Training Batch Loss: 2.723862886428833\n",
      "\t Training Batch Loss: 2.6826939582824707\n",
      "\t Training Batch Loss: 2.5203187465667725\n",
      "\t Training Batch Loss: 2.6420326232910156\n",
      "\t Training Batch Loss: 2.6398112773895264\n",
      "\t Training Batch Loss: 2.6251890659332275\n",
      "\t Training Batch Loss: 2.4934446811676025\n",
      "\t Training Batch Loss: 2.520782709121704\n",
      "\t Training Batch Loss: 2.4779276847839355\n",
      "\t Training Batch Loss: 2.558877944946289\n",
      "\t Training Batch Loss: 2.771860122680664\n",
      "\t Training Batch Loss: 3.0640811920166016\n",
      "\t Training Batch Loss: 3.0098841190338135\n",
      "\t Training Batch Loss: 2.717019557952881\n",
      "\t Training Batch Loss: 2.889695882797241\n",
      "\t Training Batch Loss: 2.7875242233276367\n",
      "\t Training Batch Loss: 2.8036229610443115\n",
      "\t Training Batch Loss: 2.63191556930542\n",
      "\t Training Batch Loss: 2.8607592582702637\n",
      "\t Training Batch Loss: 2.8241465091705322\n",
      "\t Training Batch Loss: 2.6676530838012695\n",
      "\t Training Batch Loss: 2.547788619995117\n",
      "\t Training Batch Loss: 2.633575439453125\n",
      "\t Training Batch Loss: 2.748192071914673\n",
      "\t Training Batch Loss: 2.6844334602355957\n",
      "\t Training Batch Loss: 2.6940863132476807\n",
      "\t Training Batch Loss: 2.700260639190674\n",
      "\t Training Batch Loss: 2.658454179763794\n",
      "\t Training Batch Loss: 2.576258420944214\n",
      "\t Training Batch Loss: 2.7323765754699707\n",
      "\t Training Batch Loss: 3.557537317276001\n",
      "\t Training Batch Loss: 4.695445537567139\n",
      "\t Training Batch Loss: 2.9443109035491943\n",
      "\t Training Batch Loss: 3.4049625396728516\n",
      "\t Training Batch Loss: 3.6228020191192627\n",
      "\t Training Batch Loss: 3.0743870735168457\n",
      "\t Training Batch Loss: 3.203165292739868\n",
      "\t Training Batch Loss: 3.441734790802002\n",
      "\t Training Batch Loss: 3.171372890472412\n",
      "\t Training Batch Loss: 3.278109073638916\n",
      "\t Training Batch Loss: 3.2384116649627686\n",
      "\t Training Batch Loss: 2.87239408493042\n",
      "\t Training Batch Loss: 3.1962807178497314\n",
      "\t Training Batch Loss: 3.2354238033294678\n",
      "\t Training Batch Loss: 2.9038078784942627\n",
      "\t Training Batch Loss: 3.1624364852905273\n",
      "\t Training Batch Loss: 3.05729603767395\n",
      "\t Training Batch Loss: 2.8898227214813232\n",
      "\t Training Batch Loss: 2.885587215423584\n",
      "\t Training Batch Loss: 2.988318681716919\n",
      "\t Training Batch Loss: 2.8474502563476562\n",
      "\t Training Batch Loss: 2.8229944705963135\n",
      "\t Training Batch Loss: 2.9521734714508057\n",
      "\t Training Batch Loss: 2.8499505519866943\n",
      "\t Training Batch Loss: 2.8443877696990967\n",
      "\t Training Batch Loss: 2.8481030464172363\n",
      "\t Training Batch Loss: 2.8481807708740234\n",
      "\t Training Batch Loss: 2.7731621265411377\n",
      "\t Training Batch Loss: 2.748849868774414\n",
      "\t Training Batch Loss: 2.7813446521759033\n",
      "\t Training Batch Loss: 2.6587390899658203\n",
      "\t Training Batch Loss: 2.8659963607788086\n",
      "\t Training Batch Loss: 2.7562944889068604\n",
      "\t Training Batch Loss: 2.700139284133911\n",
      "\t Training Batch Loss: 2.687257766723633\n",
      "\t Training Batch Loss: 2.7869362831115723\n",
      "\t Training Batch Loss: 2.7509102821350098\n",
      "\t Training Batch Loss: 2.7238967418670654\n",
      "\t Training Batch Loss: 2.83345365524292\n",
      "\t Training Batch Loss: 2.7586159706115723\n",
      "\t Training Batch Loss: 2.6174356937408447\n",
      "\t Training Batch Loss: 2.6830530166625977\n",
      "\t Training Batch Loss: 2.583411455154419\n",
      "\t Training Batch Loss: 2.6751394271850586\n",
      "\t Training Batch Loss: 2.565228223800659\n",
      "\t Training Batch Loss: 2.6484529972076416\n",
      "\t Training Batch Loss: 2.659677505493164\n",
      "\t Training Batch Loss: 2.5880696773529053\n",
      "\t Training Batch Loss: 2.6405348777770996\n",
      "\t Training Batch Loss: 2.669912576675415\n",
      "\t Training Batch Loss: 2.6623752117156982\n",
      "\t Training Batch Loss: 2.7056751251220703\n",
      "\t Training Batch Loss: 2.8067939281463623\n",
      "\t Training Batch Loss: 3.0648486614227295\n",
      "\t Training Batch Loss: 2.6621038913726807\n",
      "\t Training Batch Loss: 2.825727939605713\n",
      "\t Training Batch Loss: 2.864542245864868\n",
      "\t Training Batch Loss: 2.6703691482543945\n",
      "\t Training Batch Loss: 2.7426514625549316\n",
      "\t Training Batch Loss: 2.8910677433013916\n",
      "\t Training Batch Loss: 2.682297468185425\n",
      "\t Training Batch Loss: 3.1721954345703125\n",
      "\t Training Batch Loss: 3.3710696697235107\n",
      "\t Training Batch Loss: 2.9865949153900146\n",
      "\t Training Batch Loss: 3.357572555541992\n",
      "\t Training Batch Loss: 2.9328250885009766\n",
      "\t Training Batch Loss: 3.0822901725769043\n",
      "\t Training Batch Loss: 3.0656723976135254\n",
      "\t Training Batch Loss: 2.9189276695251465\n",
      "\t Training Batch Loss: 3.082967519760132\n",
      "\t Training Batch Loss: 2.775643825531006\n",
      "\t Training Batch Loss: 2.9319379329681396\n",
      "\t Training Batch Loss: 2.8824448585510254\n",
      "\t Training Batch Loss: 2.888408660888672\n",
      "\t Training Batch Loss: 2.7236716747283936\n",
      "\t Training Batch Loss: 2.7970752716064453\n",
      "\t Training Batch Loss: 2.7477447986602783\n",
      "\t Training Batch Loss: 2.7251789569854736\n",
      "\t Training Batch Loss: 2.742767095565796\n",
      "\t Training Batch Loss: 2.680206537246704\n",
      "\t Training Batch Loss: 2.77947735786438\n",
      "\t Training Batch Loss: 2.754075288772583\n",
      "\t Training Batch Loss: 2.7432620525360107\n",
      "\t Training Batch Loss: 2.515139579772949\n",
      "\t Training Batch Loss: 2.5701375007629395\n",
      "\t Training Batch Loss: 2.602501630783081\n",
      "\t Training Batch Loss: 2.4759578704833984\n",
      "\t Training Batch Loss: 2.570077896118164\n",
      "\t Training Batch Loss: 2.528320074081421\n",
      "\t Training Batch Loss: 2.5579326152801514\n",
      "\t Training Batch Loss: 2.5633814334869385\n",
      "\t Training Batch Loss: 2.5310065746307373\n",
      "\t Training Batch Loss: 2.6112329959869385\n",
      "\t Training Batch Loss: 2.5995936393737793\n",
      "\t Training Batch Loss: 2.493839740753174\n",
      "\t Training Batch Loss: 2.447547197341919\n",
      "\t Training Batch Loss: 2.546908378601074\n",
      "\t Training Batch Loss: 2.5491554737091064\n",
      "\t Training Batch Loss: 2.5004491806030273\n",
      "\t Training Batch Loss: 2.4534597396850586\n",
      "\t Training Batch Loss: 2.49664568901062\n",
      "\t Training Batch Loss: 2.475450277328491\n",
      "\t Training Batch Loss: 2.4855856895446777\n",
      "\t Training Batch Loss: 2.5015451908111572\n",
      "\t Training Batch Loss: 2.5016028881073\n",
      "\t Training Batch Loss: 2.6422367095947266\n",
      "\t Training Batch Loss: 2.781393527984619\n",
      "\t Training Batch Loss: 2.575701951980591\n",
      "\t Training Batch Loss: 2.5167012214660645\n",
      "\t Training Batch Loss: 2.6857004165649414\n",
      "\t Training Batch Loss: 2.6840577125549316\n",
      "\t Training Batch Loss: 2.578281879425049\n",
      "\t Training Batch Loss: 2.5979340076446533\n",
      "\t Training Batch Loss: 2.62101674079895\n",
      "\t Training Batch Loss: 2.7957584857940674\n",
      "\t Training Batch Loss: 2.651095390319824\n",
      "\t Training Batch Loss: 2.8313639163970947\n",
      "\t Training Batch Loss: 2.7753171920776367\n",
      "\t Training Batch Loss: 2.7034788131713867\n",
      "\t Training Batch Loss: 2.6836512088775635\n",
      "\t Training Batch Loss: 2.6424026489257812\n",
      "\t Training Batch Loss: 2.6120054721832275\n",
      "\t Training Batch Loss: 2.698939323425293\n",
      "\t Training Batch Loss: 2.5969109535217285\n",
      "\t Training Batch Loss: 2.733483076095581\n",
      "\t Training Batch Loss: 3.1231961250305176\n",
      "\t Training Batch Loss: 2.9171676635742188\n",
      "\t Training Batch Loss: 2.4879255294799805\n",
      "\t Training Batch Loss: 2.7603707313537598\n",
      "\t Training Batch Loss: 2.7724194526672363\n",
      "\t Training Batch Loss: 2.6735494136810303\n",
      "\t Training Batch Loss: 2.532904624938965\n",
      "\t Training Batch Loss: 2.6812970638275146\n",
      "\t Training Batch Loss: 2.579694986343384\n",
      "\t Training Batch Loss: 2.6200060844421387\n",
      "\t Training Batch Loss: 2.759058952331543\n",
      "\t Training Batch Loss: 2.704678773880005\n",
      "\t Training Batch Loss: 2.660006523132324\n",
      "\t Training Batch Loss: 2.8828787803649902\n",
      "\t Training Batch Loss: 2.688121795654297\n",
      "\t Training Batch Loss: 2.574965476989746\n",
      "\t Training Batch Loss: 2.7566325664520264\n",
      "\t Training Batch Loss: 2.605783700942993\n",
      "\t Training Batch Loss: 2.630751132965088\n",
      "\t Training Batch Loss: 2.5643603801727295\n",
      "\t Training Batch Loss: 2.704580068588257\n",
      "\t Training Batch Loss: 2.7157483100891113\n",
      "\t Training Batch Loss: 2.7135660648345947\n",
      "\t Training Batch Loss: 2.5963032245635986\n",
      "\t Training Batch Loss: 2.6754186153411865\n",
      "\t Training Batch Loss: 2.8284547328948975\n",
      "\t Training Batch Loss: 2.711027145385742\n",
      "\t Training Batch Loss: 2.5984082221984863\n",
      "\t Training Batch Loss: 2.6443896293640137\n",
      "\t Training Batch Loss: 2.600691556930542\n",
      "\t Training Batch Loss: 2.5692670345306396\n",
      "\t Training Batch Loss: 2.617664098739624\n",
      "\t Training Batch Loss: 2.5637965202331543\n",
      "\t Training Batch Loss: 2.567054271697998\n",
      "\t Training Batch Loss: 2.5196444988250732\n",
      "\t Training Batch Loss: 2.662558078765869\n",
      "\t Training Batch Loss: 2.573194980621338\n",
      "\t Training Batch Loss: 2.5845322608947754\n",
      "\t Training Batch Loss: 2.5335700511932373\n",
      "\t Training Batch Loss: 2.3884706497192383\n",
      "\t Training Batch Loss: 2.5679104328155518\n",
      "\t Training Batch Loss: 2.5847578048706055\n",
      "\t Training Batch Loss: 2.4994189739227295\n",
      "\t Training Batch Loss: 2.6015114784240723\n",
      "\t Training Batch Loss: 2.506701707839966\n",
      "\t Training Batch Loss: 2.4712331295013428\n",
      "\t Training Batch Loss: 2.604342460632324\n",
      "\t Training Batch Loss: 2.802156448364258\n",
      "\t Training Batch Loss: 2.7650883197784424\n",
      "\t Training Batch Loss: 2.52801251411438\n",
      "\t Training Batch Loss: 2.555929660797119\n",
      "\t Training Batch Loss: 2.5265495777130127\n",
      "\t Training Batch Loss: 2.5117685794830322\n",
      "\t Training Batch Loss: 2.6448264122009277\n",
      "\t Training Batch Loss: 2.5322465896606445\n",
      "\t Training Batch Loss: 2.506293773651123\n",
      "\t Training Batch Loss: 2.4995968341827393\n",
      "\t Training Batch Loss: 2.4912092685699463\n",
      "\t Training Batch Loss: 2.516193151473999\n",
      "\t Training Batch Loss: 2.6135189533233643\n",
      "\t Training Batch Loss: 2.5002975463867188\n",
      "\t Training Batch Loss: 2.4446074962615967\n",
      "\t Training Batch Loss: 2.632805109024048\n",
      "\t Training Batch Loss: 2.690481662750244\n",
      "\t Training Batch Loss: 2.4730911254882812\n",
      "\t Training Batch Loss: 2.6408512592315674\n",
      "\t Training Batch Loss: 2.526214361190796\n",
      "\t Training Batch Loss: 2.4610297679901123\n",
      "\t Training Batch Loss: 2.4954941272735596\n",
      "\t Training Batch Loss: 2.4806644916534424\n",
      "\t Training Batch Loss: 2.665090322494507\n",
      "\t Training Batch Loss: 2.8194775581359863\n",
      "\t Training Batch Loss: 2.7203409671783447\n",
      "\t Training Batch Loss: 2.52264404296875\n",
      "\t Training Batch Loss: 2.7709150314331055\n",
      "\t Training Batch Loss: 2.648621082305908\n",
      "\t Training Batch Loss: 2.6373414993286133\n",
      "\t Training Batch Loss: 2.5995261669158936\n",
      "\t Training Batch Loss: 2.502164602279663\n",
      "\t Training Batch Loss: 2.6388895511627197\n",
      "\t Training Batch Loss: 2.634424924850464\n",
      "\t Training Batch Loss: 2.6070942878723145\n",
      "\t Training Batch Loss: 2.5366005897521973\n",
      "\t Training Batch Loss: 2.7570443153381348\n",
      "\t Training Batch Loss: 2.6600747108459473\n",
      "\t Training Batch Loss: 2.63633394241333\n",
      "\t Training Batch Loss: 2.5863561630249023\n",
      "\t Training Batch Loss: 2.7027394771575928\n",
      "\t Training Batch Loss: 2.4775795936584473\n",
      "\t Training Batch Loss: 2.706547498703003\n",
      "\t Training Batch Loss: 2.7287344932556152\n",
      "\t Training Batch Loss: 2.565678119659424\n",
      "\t Training Batch Loss: 2.752146005630493\n",
      "\t Training Batch Loss: 2.7245888710021973\n",
      "\t Training Batch Loss: 2.5909292697906494\n",
      "\t Training Batch Loss: 2.5492208003997803\n",
      "\t Training Batch Loss: 2.6917717456817627\n",
      "\t Training Batch Loss: 2.664113998413086\n",
      "\t Training Batch Loss: 2.6124420166015625\n",
      "\t Training Batch Loss: 2.586268424987793\n",
      "\t Training Batch Loss: 2.5524704456329346\n",
      "\t Training Batch Loss: 2.511850118637085\n",
      "\t Training Batch Loss: 2.4997398853302\n",
      "\t Training Batch Loss: 2.515582323074341\n",
      "\t Training Batch Loss: 2.5278544425964355\n",
      "\t Training Batch Loss: 2.5491397380828857\n",
      "\t Training Batch Loss: 2.560605764389038\n",
      "\t Training Batch Loss: 2.514490842819214\n",
      "\t Training Batch Loss: 2.579759120941162\n",
      "\t Training Batch Loss: 2.6300508975982666\n",
      "\t Training Batch Loss: 2.772897720336914\n",
      "\t Training Batch Loss: 2.659226894378662\n",
      "\t Training Batch Loss: 2.7462334632873535\n",
      "\t Training Batch Loss: 2.6033437252044678\n",
      "\t Training Batch Loss: 2.5015125274658203\n",
      "\t Training Batch Loss: 2.4343321323394775\n",
      "\t Training Batch Loss: 2.546527624130249\n",
      "\t Training Batch Loss: 2.540372371673584\n",
      "\t Training Batch Loss: 2.4729366302490234\n",
      "\t Training Batch Loss: 2.4083359241485596\n",
      "\t Training Batch Loss: 2.6109066009521484\n",
      "\t Training Batch Loss: 2.5079739093780518\n",
      "\t Training Batch Loss: 2.731855630874634\n",
      "\t Training Batch Loss: 2.7508621215820312\n",
      "\t Training Batch Loss: 2.4700374603271484\n",
      "\t Training Batch Loss: 2.6237571239471436\n",
      "\t Training Batch Loss: 2.5401017665863037\n",
      "\t Training Batch Loss: 2.5632708072662354\n",
      "\t Training Batch Loss: 2.7145910263061523\n",
      "\t Training Batch Loss: 2.560638666152954\n",
      "\t Training Batch Loss: 2.5596439838409424\n",
      "\t Training Batch Loss: 2.4921910762786865\n",
      "\t Training Batch Loss: 2.6623117923736572\n",
      "\t Training Batch Loss: 2.560197591781616\n",
      "\t Training Batch Loss: 2.553056240081787\n",
      "\t Training Batch Loss: 2.5137619972229004\n",
      "\t Training Batch Loss: 2.4807517528533936\n",
      "\t Training Batch Loss: 2.5048623085021973\n",
      "\t Training Batch Loss: 2.405698299407959\n",
      "\t Training Batch Loss: 2.6128499507904053\n",
      "\t Training Batch Loss: 2.655721426010132\n",
      "\t Training Batch Loss: 2.5521790981292725\n",
      "\t Training Batch Loss: 2.563979387283325\n",
      "\t Training Batch Loss: 2.6133298873901367\n",
      "\t Training Batch Loss: 2.646467685699463\n",
      "\t Training Batch Loss: 2.601243257522583\n",
      "\t Training Batch Loss: 2.668203592300415\n",
      "\t Training Batch Loss: 2.547358751296997\n",
      "\t Training Batch Loss: 2.752781867980957\n",
      "\t Training Batch Loss: 2.6438817977905273\n",
      "\t Training Batch Loss: 2.4962027072906494\n",
      "\t Training Batch Loss: 2.6987175941467285\n",
      "\t Training Batch Loss: 2.616581916809082\n",
      "\t Training Batch Loss: 2.391378402709961\n",
      "\t Training Batch Loss: 2.6930224895477295\n",
      "\t Training Batch Loss: 2.7200379371643066\n",
      "\t Training Batch Loss: 2.4084455966949463\n",
      "\t Training Batch Loss: 2.632282018661499\n",
      "\t Training Batch Loss: 2.665994644165039\n",
      "\t Training Batch Loss: 2.536478042602539\n",
      "\t Training Batch Loss: 2.778867244720459\n",
      "\t Training Batch Loss: 2.4452455043792725\n",
      "\t Training Batch Loss: 2.6735899448394775\n",
      "\t Training Batch Loss: 2.648635149002075\n",
      "\t Training Batch Loss: 2.5247693061828613\n",
      "\t Training Batch Loss: 2.674938201904297\n",
      "\t Training Batch Loss: 2.641862630844116\n",
      "\t Training Batch Loss: 2.6554923057556152\n",
      "\t Training Batch Loss: 2.6294760704040527\n",
      "\t Training Batch Loss: 2.615814208984375\n",
      "\t Training Batch Loss: 2.5460386276245117\n",
      "\t Training Batch Loss: 2.597290277481079\n",
      "\t Training Batch Loss: 2.6170787811279297\n",
      "\t Training Batch Loss: 2.647900104522705\n",
      "\t Training Batch Loss: 2.5429561138153076\n",
      "\t Training Batch Loss: 2.5422441959381104\n",
      "\t Training Batch Loss: 2.4787895679473877\n",
      "\t Training Batch Loss: 2.6539738178253174\n",
      "\t Training Batch Loss: 2.7250611782073975\n",
      "\t Training Batch Loss: 2.832810878753662\n",
      "\t Training Batch Loss: 2.6265676021575928\n",
      "\t Training Batch Loss: 2.528602123260498\n",
      "\t Training Batch Loss: 2.737870454788208\n",
      "\t Training Batch Loss: 2.528933525085449\n",
      "\t Training Batch Loss: 2.7433369159698486\n",
      "\t Training Batch Loss: 2.629570722579956\n",
      "\t Training Batch Loss: 2.559633493423462\n",
      "\t Training Batch Loss: 2.695704698562622\n",
      "\t Training Batch Loss: 2.8292055130004883\n",
      "\t Training Batch Loss: 2.75846004486084\n",
      "\t Training Batch Loss: 2.589538812637329\n",
      "\t Training Batch Loss: 2.753535270690918\n",
      "\t Training Batch Loss: 2.5860044956207275\n",
      "\t Training Batch Loss: 2.608508825302124\n",
      "\t Training Batch Loss: 2.5303854942321777\n",
      "\t Training Batch Loss: 2.5090200901031494\n",
      "\t Training Batch Loss: 2.5440053939819336\n",
      "\t Training Batch Loss: 2.41949200630188\n",
      "\t Training Batch Loss: 2.4265787601470947\n",
      "\t Training Batch Loss: 2.540518283843994\n",
      "\t Training Batch Loss: 2.5779457092285156\n",
      "\t Training Batch Loss: 2.582521915435791\n",
      "\t Training Batch Loss: 2.4343364238739014\n",
      "\t Training Batch Loss: 2.5100491046905518\n",
      "\t Training Batch Loss: 2.526607036590576\n",
      "\t Training Batch Loss: 2.555316209793091\n",
      "\t Training Batch Loss: 2.5099871158599854\n",
      "\t Training Batch Loss: 2.611766815185547\n",
      "\t Training Batch Loss: 2.4469761848449707\n",
      "\t Training Batch Loss: 2.5111188888549805\n",
      "\t Training Batch Loss: 2.535557746887207\n",
      "\t Training Batch Loss: 2.6426942348480225\n",
      "\t Training Batch Loss: 2.651583194732666\n",
      "\t Training Batch Loss: 2.4940779209136963\n",
      "\t Training Batch Loss: 2.4680702686309814\n",
      "\t Training Batch Loss: 2.5038349628448486\n",
      "\t Training Batch Loss: 2.45070219039917\n",
      "\t Training Batch Loss: 2.6070914268493652\n",
      "\t Training Batch Loss: 2.717130422592163\n",
      "\t Training Batch Loss: 2.655775785446167\n",
      "\t Training Batch Loss: 2.4621338844299316\n",
      "\t Training Batch Loss: 2.6344096660614014\n",
      "\t Training Batch Loss: 2.5274817943573\n",
      "\t Training Batch Loss: 2.582829713821411\n",
      "\t Training Batch Loss: 2.638437509536743\n",
      "\t Training Batch Loss: 2.5691096782684326\n",
      "\t Training Batch Loss: 2.515484571456909\n",
      "\t Training Batch Loss: 2.546685218811035\n",
      "\t Training Batch Loss: 2.617626905441284\n",
      "\t Training Batch Loss: 2.6213977336883545\n",
      "\t Training Batch Loss: 2.5046539306640625\n",
      "\t Training Batch Loss: 2.5412468910217285\n",
      "\t Training Batch Loss: 2.586160182952881\n",
      "\t Training Batch Loss: 2.6207845211029053\n",
      "\t Training Batch Loss: 2.543088436126709\n",
      "\t Training Batch Loss: 2.751332998275757\n",
      "\t Training Batch Loss: 2.962233066558838\n",
      "\t Training Batch Loss: 2.7514138221740723\n",
      "\t Training Batch Loss: 2.5980303287506104\n",
      "\t Training Batch Loss: 2.9230971336364746\n",
      "\t Training Batch Loss: 2.7613747119903564\n",
      "\t Training Batch Loss: 2.69918155670166\n",
      "\t Training Batch Loss: 2.689056873321533\n",
      "\t Training Batch Loss: 2.784956455230713\n",
      "\t Training Batch Loss: 2.7856316566467285\n",
      "\t Training Batch Loss: 2.6379711627960205\n",
      "\t Training Batch Loss: 2.9244296550750732\n",
      "\t Training Batch Loss: 2.8008196353912354\n",
      "\t Training Batch Loss: 2.5988192558288574\n",
      "\t Training Batch Loss: 2.618537425994873\n",
      "\t Training Batch Loss: 2.6707866191864014\n",
      "\t Training Batch Loss: 2.5310115814208984\n",
      "\t Training Batch Loss: 2.550351619720459\n",
      "\t Training Batch Loss: 2.6029701232910156\n",
      "\t Training Batch Loss: 2.637901544570923\n",
      "\t Training Batch Loss: 2.5791678428649902\n",
      "\t Training Batch Loss: 2.6600253582000732\n",
      "\t Training Batch Loss: 2.7191991806030273\n",
      "\t Training Batch Loss: 2.7199158668518066\n",
      "\t Training Batch Loss: 2.5264925956726074\n",
      "\t Training Batch Loss: 2.65055513381958\n",
      "\t Training Batch Loss: 2.5478827953338623\n",
      "\t Training Batch Loss: 2.600409984588623\n",
      "\t Training Batch Loss: 2.605297565460205\n",
      "\t Training Batch Loss: 2.662386655807495\n",
      "\t Training Batch Loss: 2.5336060523986816\n",
      "\t Training Batch Loss: 2.7051820755004883\n",
      "\t Training Batch Loss: 3.1265153884887695\n",
      "\t Training Batch Loss: 2.9693243503570557\n",
      "\t Training Batch Loss: 2.6121275424957275\n",
      "\t Training Batch Loss: 2.6573405265808105\n",
      "\t Training Batch Loss: 2.6047921180725098\n",
      "\t Training Batch Loss: 2.795130491256714\n",
      "\t Training Batch Loss: 2.6220543384552\n",
      "\t Training Batch Loss: 2.6011741161346436\n",
      "\t Training Batch Loss: 2.489814519882202\n",
      "\t Training Batch Loss: 2.625441789627075\n",
      "\t Training Batch Loss: 2.7752201557159424\n",
      "\t Training Batch Loss: 2.558619737625122\n",
      "\t Training Batch Loss: 2.528759717941284\n",
      "\t Training Batch Loss: 2.598414421081543\n",
      "\t Training Batch Loss: 2.5414440631866455\n",
      "\t Training Batch Loss: 2.623450756072998\n",
      "\t Training Batch Loss: 2.4159815311431885\n",
      "\t Training Batch Loss: 2.5962536334991455\n",
      "\t Training Batch Loss: 2.4691619873046875\n",
      "\t Training Batch Loss: 2.432594060897827\n",
      "\t Training Batch Loss: 2.4168615341186523\n",
      "\t Training Batch Loss: 2.6007637977600098\n",
      "\t Training Batch Loss: 2.758646011352539\n",
      "\t Training Batch Loss: 2.599276065826416\n",
      "\t Training Batch Loss: 2.436702251434326\n",
      "\t Training Batch Loss: 2.5615625381469727\n",
      "\t Training Batch Loss: 2.489168405532837\n",
      "\t Training Batch Loss: 2.540381908416748\n",
      "\t Training Batch Loss: 2.4636049270629883\n",
      "\t Training Batch Loss: 2.4703173637390137\n",
      "\t Training Batch Loss: 2.628793954849243\n",
      "\t Training Batch Loss: 2.6471660137176514\n",
      "\t Training Batch Loss: 2.5045087337493896\n",
      "\t Training Batch Loss: 2.486990213394165\n",
      "\t Training Batch Loss: 2.5463459491729736\n",
      "\t Training Batch Loss: 2.5117366313934326\n",
      "\t Training Batch Loss: 2.5842506885528564\n",
      "\t Training Batch Loss: 2.4066169261932373\n",
      "\t Training Batch Loss: 2.487544536590576\n",
      "\t Training Batch Loss: 2.60042667388916\n",
      "\t Training Batch Loss: 2.696528911590576\n",
      "\t Training Batch Loss: 2.503850221633911\n",
      "\t Training Batch Loss: 2.408527374267578\n",
      "\t Training Batch Loss: 2.4470629692077637\n",
      "\t Training Batch Loss: 2.544215440750122\n",
      "\t Training Batch Loss: 2.52911114692688\n",
      "\t Training Batch Loss: 2.5041675567626953\n",
      "\t Training Batch Loss: 2.4847452640533447\n",
      "\t Training Batch Loss: 2.5488264560699463\n",
      "\t Training Batch Loss: 2.575221061706543\n",
      "\t Training Batch Loss: 2.4514496326446533\n",
      "\t Training Batch Loss: 2.5297272205352783\n",
      "\t Training Batch Loss: 2.505601406097412\n",
      "\t Training Batch Loss: 2.484776496887207\n",
      "\t Training Batch Loss: 2.5732786655426025\n",
      "\t Training Batch Loss: 2.7096915245056152\n",
      "\t Training Batch Loss: 2.649448871612549\n",
      "\t Training Batch Loss: 2.6629669666290283\n",
      "\t Training Batch Loss: 2.6598572731018066\n",
      "\t Training Batch Loss: 2.602818012237549\n",
      "\t Training Batch Loss: 2.677365303039551\n",
      "\t Training Batch Loss: 2.551084041595459\n",
      "\t Training Batch Loss: 2.5183660984039307\n",
      "\t Training Batch Loss: 2.4666688442230225\n",
      "\t Training Batch Loss: 2.5431764125823975\n",
      "\t Training Batch Loss: 2.4351894855499268\n",
      "\t Training Batch Loss: 2.573265790939331\n",
      "\t Training Batch Loss: 2.5246005058288574\n",
      "\t Training Batch Loss: 2.609076976776123\n",
      "\t Training Batch Loss: 2.593977212905884\n",
      "\t Training Batch Loss: 2.5251524448394775\n",
      "\t Training Batch Loss: 2.5715348720550537\n",
      "\t Training Batch Loss: 2.465298652648926\n",
      "\t Training Batch Loss: 2.689854860305786\n",
      "\t Training Batch Loss: 3.130563259124756\n",
      "\t Training Batch Loss: 4.940226078033447\n",
      "\t Training Batch Loss: 4.5593180656433105\n",
      "\t Training Batch Loss: 3.676992893218994\n",
      "\t Training Batch Loss: 3.7901999950408936\n",
      "\t Training Batch Loss: 3.739088535308838\n",
      "\t Training Batch Loss: 3.5296921730041504\n",
      "\t Training Batch Loss: 3.508445978164673\n",
      "\t Training Batch Loss: 3.453472137451172\n",
      "\t Training Batch Loss: 3.5959012508392334\n",
      "\t Training Batch Loss: 3.543856382369995\n",
      "\t Training Batch Loss: 3.5626721382141113\n",
      "\t Training Batch Loss: 3.3466906547546387\n",
      "\t Training Batch Loss: 3.33992862701416\n",
      "\t Training Batch Loss: 3.4073173999786377\n",
      "\t Training Batch Loss: 3.6173698902130127\n",
      "\t Training Batch Loss: 3.3538219928741455\n",
      "\t Training Batch Loss: 3.655550003051758\n",
      "\t Training Batch Loss: 3.3571078777313232\n",
      "\t Training Batch Loss: 3.249052047729492\n",
      "\t Training Batch Loss: 3.4510154724121094\n",
      "\t Training Batch Loss: 3.0714032649993896\n",
      "\t Training Batch Loss: 3.308629035949707\n",
      "\t Training Batch Loss: 3.2777907848358154\n",
      "\t Training Batch Loss: 3.18900728225708\n",
      "\t Training Batch Loss: 3.353823661804199\n",
      "\t Training Batch Loss: 3.2126224040985107\n",
      "\t Training Batch Loss: 3.096357583999634\n",
      "\t Training Batch Loss: 3.159727096557617\n",
      "\t Training Batch Loss: 3.2257978916168213\n",
      "\t Training Batch Loss: 3.080005407333374\n",
      "\t Training Batch Loss: 2.9725661277770996\n",
      "\t Training Batch Loss: 2.977667808532715\n",
      "\t Training Batch Loss: 2.921515703201294\n",
      "\t Training Batch Loss: 2.8988523483276367\n",
      "\t Training Batch Loss: 2.8301117420196533\n",
      "\t Training Batch Loss: 3.059391736984253\n",
      "\t Training Batch Loss: 3.50829815864563\n",
      "\t Training Batch Loss: 3.0489115715026855\n",
      "\t Training Batch Loss: 3.1773200035095215\n",
      "\t Training Batch Loss: 2.9599640369415283\n",
      "\t Training Batch Loss: 2.967409133911133\n",
      "\t Training Batch Loss: 2.98746657371521\n",
      "\t Training Batch Loss: 2.977285385131836\n",
      "\t Training Batch Loss: 3.011490821838379\n",
      "\t Training Batch Loss: 2.9843668937683105\n",
      "\t Training Batch Loss: 2.8875062465667725\n",
      "\t Training Batch Loss: 2.8898403644561768\n",
      "\t Training Batch Loss: 2.831549644470215\n",
      "\t Training Batch Loss: 2.962559461593628\n",
      "\t Training Batch Loss: 2.7893691062927246\n",
      "\t Training Batch Loss: 2.8325953483581543\n",
      "\t Training Batch Loss: 2.8698647022247314\n",
      "\t Training Batch Loss: 2.7540628910064697\n",
      "\t Training Batch Loss: 2.720310688018799\n",
      "\t Training Batch Loss: 2.8671581745147705\n",
      "\t Training Batch Loss: 2.705612897872925\n",
      "\t Training Batch Loss: 2.8098275661468506\n",
      "\t Training Batch Loss: 2.751429557800293\n",
      "\t Training Batch Loss: 2.795220136642456\n",
      "\t Training Batch Loss: 2.7223150730133057\n",
      "\t Training Batch Loss: 2.803224802017212\n",
      "\t Training Batch Loss: 2.8341243267059326\n",
      "\t Training Batch Loss: 2.8453760147094727\n",
      "\t Training Batch Loss: 3.009676218032837\n",
      "\t Training Batch Loss: 2.956589937210083\n",
      "\t Training Batch Loss: 2.7051610946655273\n",
      "\t Training Batch Loss: 2.7011754512786865\n",
      "\t Training Batch Loss: 2.916247844696045\n",
      "\t Training Batch Loss: 2.7512357234954834\n",
      "\t Training Batch Loss: 2.6026196479797363\n",
      "\t Training Batch Loss: 2.767143964767456\n",
      "\t Training Batch Loss: 2.8616371154785156\n",
      "\t Training Batch Loss: 3.0488526821136475\n",
      "\t Training Batch Loss: 2.9980621337890625\n",
      "\t Training Batch Loss: 2.714136838912964\n",
      "\t Training Batch Loss: 2.805238723754883\n",
      "\t Training Batch Loss: 2.876973867416382\n",
      "\t Training Batch Loss: 2.7585320472717285\n",
      "\t Training Batch Loss: 2.845083713531494\n",
      "\t Training Batch Loss: 2.6784019470214844\n",
      "\t Training Batch Loss: 2.6937921047210693\n",
      "\t Training Batch Loss: 2.8738842010498047\n",
      "\t Training Batch Loss: 2.9916021823883057\n",
      "\t Training Batch Loss: 2.875657081604004\n",
      "\t Training Batch Loss: 2.7100558280944824\n",
      "\t Training Batch Loss: 2.6584057807922363\n",
      "\t Training Batch Loss: 2.861204147338867\n",
      "\t Training Batch Loss: 2.649665594100952\n",
      "\t Training Batch Loss: 2.765098810195923\n",
      "\t Training Batch Loss: 2.6096367835998535\n",
      "\t Training Batch Loss: 2.558823585510254\n",
      "\t Training Batch Loss: 2.6384286880493164\n",
      "\t Training Batch Loss: 2.6166696548461914\n",
      "\t Training Batch Loss: 2.5428929328918457\n",
      "\t Training Batch Loss: 2.6231236457824707\n",
      "\t Training Batch Loss: 2.7421247959136963\n",
      "\t Training Batch Loss: 2.6803412437438965\n",
      "\t Training Batch Loss: 2.7029917240142822\n",
      "\t Training Batch Loss: 2.6538586616516113\n",
      "\t Training Batch Loss: 2.5900750160217285\n",
      "\t Training Batch Loss: 2.717618465423584\n",
      "\t Training Batch Loss: 2.549513816833496\n",
      "\t Training Batch Loss: 2.6561222076416016\n",
      "\t Training Batch Loss: 2.839242696762085\n",
      "\t Training Batch Loss: 2.773866891860962\n",
      "\t Training Batch Loss: 2.501563549041748\n",
      "\t Training Batch Loss: 2.6894657611846924\n",
      "\t Training Batch Loss: 2.5626909732818604\n",
      "\t Training Batch Loss: 2.521867275238037\n",
      "\t Training Batch Loss: 2.5902271270751953\n",
      "\t Training Batch Loss: 2.524134397506714\n",
      "\t Training Batch Loss: 2.5449559688568115\n",
      "\t Training Batch Loss: 2.6754934787750244\n",
      "\t Training Batch Loss: 2.6564321517944336\n",
      "\t Training Batch Loss: 2.7148406505584717\n",
      "\t Training Batch Loss: 2.6004958152770996\n",
      "\t Training Batch Loss: 2.580657482147217\n",
      "\t Training Batch Loss: 2.589202404022217\n",
      "\t Training Batch Loss: 2.674730062484741\n",
      "\t Training Batch Loss: 2.6521079540252686\n",
      "\t Training Batch Loss: 2.608041763305664\n",
      "\t Training Batch Loss: 2.6892545223236084\n",
      "\t Training Batch Loss: 2.7574002742767334\n",
      "\t Training Batch Loss: 2.635873556137085\n",
      "\t Training Batch Loss: 2.553157091140747\n",
      "\t Training Batch Loss: 2.5762574672698975\n",
      "\t Training Batch Loss: 2.5855255126953125\n",
      "\t Training Batch Loss: 2.531848430633545\n",
      "\t Training Batch Loss: 2.688944101333618\n",
      "\t Training Batch Loss: 2.6273863315582275\n",
      "\t Training Batch Loss: 2.601154088973999\n",
      "\t Training Batch Loss: 2.5482258796691895\n",
      "\t Training Batch Loss: 2.5343358516693115\n",
      "\t Training Batch Loss: 2.792597770690918\n",
      "\t Training Batch Loss: 2.6875381469726562\n",
      "\t Training Batch Loss: 2.6662352085113525\n",
      "\t Training Batch Loss: 2.61525559425354\n",
      "\t Training Batch Loss: 2.667606830596924\n",
      "\t Training Batch Loss: 2.646317958831787\n",
      "\t Training Batch Loss: 2.6046805381774902\n",
      "\t Training Batch Loss: 2.7850186824798584\n",
      "\t Training Batch Loss: 2.8969037532806396\n",
      "\t Training Batch Loss: 2.70428204536438\n",
      "\t Training Batch Loss: 2.7249484062194824\n",
      "\t Training Batch Loss: 2.6729915142059326\n",
      "\t Training Batch Loss: 2.780977964401245\n",
      "\t Training Batch Loss: 2.6002392768859863\n",
      "\t Training Batch Loss: 2.597440004348755\n",
      "\t Training Batch Loss: 2.77052903175354\n",
      "\t Training Batch Loss: 2.679252862930298\n",
      "\t Training Batch Loss: 2.5643014907836914\n",
      "\t Training Batch Loss: 2.5885684490203857\n",
      "\t Training Batch Loss: 2.725879669189453\n",
      "\t Training Batch Loss: 2.5958123207092285\n",
      "\t Training Batch Loss: 2.556922674179077\n",
      "\t Training Batch Loss: 2.5814716815948486\n",
      "\t Training Batch Loss: 2.6619272232055664\n",
      "\t Training Batch Loss: 2.7763912677764893\n",
      "\t Training Batch Loss: 2.6921284198760986\n",
      "\t Training Batch Loss: 2.7925617694854736\n",
      "\t Training Batch Loss: 2.744317054748535\n",
      "\t Training Batch Loss: 2.6801748275756836\n",
      "\t Training Batch Loss: 2.5272223949432373\n",
      "\t Training Batch Loss: 2.5346970558166504\n",
      "\t Training Batch Loss: 2.40213680267334\n",
      "\t Training Batch Loss: 2.4702446460723877\n",
      "\t Training Batch Loss: 2.5661802291870117\n",
      "\t Training Batch Loss: 2.4347305297851562\n",
      "\t Training Batch Loss: 2.4669086933135986\n",
      "\t Training Batch Loss: 2.5174076557159424\n",
      "\t Training Batch Loss: 2.542531967163086\n",
      "\t Training Batch Loss: 2.5260510444641113\n",
      "\t Training Batch Loss: 2.4164535999298096\n",
      "\t Training Batch Loss: 2.446007251739502\n",
      "\t Training Batch Loss: 2.4270615577697754\n",
      "\t Training Batch Loss: 2.4629886150360107\n",
      "\t Training Batch Loss: 2.3686270713806152\n",
      "\t Training Batch Loss: 2.420232057571411\n",
      "\t Training Batch Loss: 2.441751480102539\n",
      "\t Training Batch Loss: 2.4519340991973877\n",
      "\t Training Batch Loss: 2.496133804321289\n",
      "\t Training Batch Loss: 2.6314857006073\n",
      "\t Training Batch Loss: 2.8192873001098633\n",
      "\t Training Batch Loss: 2.581470489501953\n",
      "\t Training Batch Loss: 2.5050203800201416\n",
      "\t Training Batch Loss: 2.603696584701538\n",
      "\t Training Batch Loss: 2.456984758377075\n",
      "\t Training Batch Loss: 2.6244473457336426\n",
      "\t Training Batch Loss: 2.6048688888549805\n",
      "\t Training Batch Loss: 2.6774730682373047\n",
      "\t Training Batch Loss: 2.606628656387329\n",
      "\t Training Batch Loss: 2.4764912128448486\n",
      "\t Training Batch Loss: 2.6295549869537354\n",
      "\t Training Batch Loss: 2.6537015438079834\n",
      "\t Training Batch Loss: 2.621424436569214\n",
      "\t Training Batch Loss: 2.6820993423461914\n",
      "\t Training Batch Loss: 2.676215410232544\n",
      "\t Training Batch Loss: 2.6366567611694336\n",
      "\t Training Batch Loss: 2.643841028213501\n",
      "\t Training Batch Loss: 2.8627922534942627\n",
      "\t Training Batch Loss: 2.536776065826416\n",
      "\t Training Batch Loss: 2.6776556968688965\n",
      "\t Training Batch Loss: 2.8053550720214844\n",
      "\t Training Batch Loss: 2.612668514251709\n",
      "\t Training Batch Loss: 2.619563579559326\n",
      "\t Training Batch Loss: 2.525092840194702\n",
      "\t Training Batch Loss: 2.589264154434204\n",
      "\t Training Batch Loss: 2.591261625289917\n",
      "\t Training Batch Loss: 2.5403521060943604\n",
      "\t Training Batch Loss: 2.5133910179138184\n",
      "\t Training Batch Loss: 2.5217573642730713\n",
      "\t Training Batch Loss: 2.5407485961914062\n",
      "\t Training Batch Loss: 2.5706605911254883\n",
      "\t Training Batch Loss: 2.5161399841308594\n",
      "\t Training Batch Loss: 2.6405017375946045\n",
      "\t Training Batch Loss: 2.586097478866577\n",
      "\t Training Batch Loss: 2.5646164417266846\n",
      "\t Training Batch Loss: 2.5045106410980225\n",
      "\t Training Batch Loss: 2.556783437728882\n",
      "\t Training Batch Loss: 2.8422346115112305\n",
      "\t Training Batch Loss: 3.586500883102417\n",
      "\t Training Batch Loss: 3.2513911724090576\n",
      "\t Training Batch Loss: 3.0306622982025146\n",
      "\t Training Batch Loss: 3.0698840618133545\n",
      "\t Training Batch Loss: 3.134026527404785\n",
      "\t Training Batch Loss: 3.066228151321411\n",
      "\t Training Batch Loss: 3.2582550048828125\n",
      "\t Training Batch Loss: 3.0186803340911865\n",
      "\t Training Batch Loss: 3.042666435241699\n",
      "\t Training Batch Loss: 2.896355152130127\n",
      "\t Training Batch Loss: 2.949167013168335\n",
      "\t Training Batch Loss: 2.847316265106201\n",
      "\t Training Batch Loss: 3.0386266708374023\n",
      "\t Training Batch Loss: 2.954681634902954\n",
      "\t Training Batch Loss: 2.8491978645324707\n",
      "\t Training Batch Loss: 2.8941352367401123\n",
      "\t Training Batch Loss: 2.697444438934326\n",
      "\t Training Batch Loss: 2.8153533935546875\n",
      "\t Training Batch Loss: 2.851776361465454\n",
      "\t Training Batch Loss: 2.662297010421753\n",
      "\t Training Batch Loss: 2.7593014240264893\n",
      "\t Training Batch Loss: 2.7938392162323\n",
      "\t Training Batch Loss: 2.6221044063568115\n",
      "\t Training Batch Loss: 2.846188545227051\n",
      "\t Training Batch Loss: 2.8422598838806152\n",
      "\t Training Batch Loss: 2.567681074142456\n",
      "\t Training Batch Loss: 2.7055885791778564\n",
      "\t Training Batch Loss: 2.730778217315674\n",
      "\t Training Batch Loss: 2.625059127807617\n",
      "\t Training Batch Loss: 2.8033645153045654\n",
      "\t Training Batch Loss: 2.6668856143951416\n",
      "\t Training Batch Loss: 2.600614070892334\n",
      "\t Training Batch Loss: 2.8260385990142822\n",
      "\t Training Batch Loss: 2.614771604537964\n",
      "\t Training Batch Loss: 2.637924909591675\n",
      "\t Training Batch Loss: 2.8159127235412598\n",
      "\t Training Batch Loss: 2.506791830062866\n",
      "\t Training Batch Loss: 2.5304341316223145\n",
      "\t Training Batch Loss: 2.7810919284820557\n",
      "\t Training Batch Loss: 2.481844186782837\n",
      "\t Training Batch Loss: 2.6423230171203613\n",
      "\t Training Batch Loss: 2.5889620780944824\n",
      "\t Training Batch Loss: 2.42806077003479\n",
      "\t Training Batch Loss: 2.506093740463257\n",
      "\t Training Batch Loss: 2.591794013977051\n",
      "\t Training Batch Loss: 2.585599184036255\n",
      "\t Training Batch Loss: 2.546903610229492\n",
      "\t Training Batch Loss: 2.563786268234253\n",
      "\t Training Batch Loss: 2.5918209552764893\n",
      "\t Training Batch Loss: 2.5515079498291016\n",
      "\t Training Batch Loss: 2.449676036834717\n",
      "\t Training Batch Loss: 2.4929041862487793\n",
      "\t Training Batch Loss: 2.5348567962646484\n",
      "\t Training Batch Loss: 2.611908435821533\n",
      "\t Training Batch Loss: 2.4692726135253906\n",
      "\t Training Batch Loss: 2.547445297241211\n",
      "\t Training Batch Loss: 2.5903778076171875\n",
      "\t Training Batch Loss: 2.7026052474975586\n",
      "\t Training Batch Loss: 2.624748706817627\n",
      "\t Training Batch Loss: 2.5107781887054443\n",
      "\t Training Batch Loss: 2.5260279178619385\n",
      "\t Training Batch Loss: 2.523893117904663\n",
      "\t Training Batch Loss: 2.5748324394226074\n",
      "\t Training Batch Loss: 2.6832456588745117\n",
      "\t Training Batch Loss: 2.9444546699523926\n",
      "\t Training Batch Loss: 2.695582866668701\n",
      "\t Training Batch Loss: 2.628950834274292\n",
      "\t Training Batch Loss: 2.726134777069092\n",
      "\t Training Batch Loss: 2.5870659351348877\n",
      "\t Training Batch Loss: 2.8860886096954346\n",
      "\t Training Batch Loss: 2.746368169784546\n",
      "\t Training Batch Loss: 2.6846096515655518\n",
      "\t Training Batch Loss: 2.9492266178131104\n",
      "\t Training Batch Loss: 2.778773069381714\n",
      "\t Training Batch Loss: 2.6130428314208984\n",
      "\t Training Batch Loss: 3.016083240509033\n",
      "\t Training Batch Loss: 2.7464981079101562\n",
      "\t Training Batch Loss: 2.78458833694458\n",
      "\t Training Batch Loss: 2.8136448860168457\n",
      "\t Training Batch Loss: 2.7865498065948486\n",
      "\t Training Batch Loss: 2.805914878845215\n",
      "\t Training Batch Loss: 2.7553675174713135\n",
      "\t Training Batch Loss: 2.7617125511169434\n",
      "\t Training Batch Loss: 2.630125045776367\n",
      "\t Training Batch Loss: 2.6745071411132812\n",
      "\t Training Batch Loss: 2.973888635635376\n",
      "\t Training Batch Loss: 2.703467607498169\n",
      "\t Training Batch Loss: 2.5434727668762207\n",
      "\t Training Batch Loss: 2.7965447902679443\n",
      "\t Training Batch Loss: 2.567561388015747\n",
      "\t Training Batch Loss: 2.6809329986572266\n",
      "\t Training Batch Loss: 2.627697229385376\n",
      "\t Training Batch Loss: 2.6698520183563232\n",
      "\t Training Batch Loss: 2.712050676345825\n",
      "\t Training Batch Loss: 2.719799041748047\n",
      "\t Training Batch Loss: 2.6077969074249268\n",
      "\t Training Batch Loss: 2.6360483169555664\n",
      "\t Training Batch Loss: 2.6329588890075684\n",
      "\t Training Batch Loss: 2.527445077896118\n",
      "\t Training Batch Loss: 2.4722495079040527\n",
      "\t Training Batch Loss: 2.6156229972839355\n",
      "\t Training Batch Loss: 2.525407552719116\n",
      "\t Training Batch Loss: 2.485244035720825\n",
      "\t Training Batch Loss: 2.418992519378662\n",
      "\t Training Batch Loss: 2.4801876544952393\n",
      "\t Training Batch Loss: 2.4152660369873047\n",
      "\t Training Batch Loss: 2.484996795654297\n",
      "\t Training Batch Loss: 2.546354055404663\n",
      "\t Training Batch Loss: 2.7658398151397705\n",
      "\t Training Batch Loss: 2.909229040145874\n",
      "\t Training Batch Loss: 2.5246689319610596\n",
      "\t Training Batch Loss: 2.7069993019104004\n",
      "\t Training Batch Loss: 2.692589521408081\n",
      "\t Training Batch Loss: 2.8167641162872314\n",
      "\t Training Batch Loss: 2.566492795944214\n",
      "\t Training Batch Loss: 2.6424450874328613\n",
      "\t Training Batch Loss: 2.701859951019287\n",
      "\t Training Batch Loss: 2.5639455318450928\n",
      "\t Training Batch Loss: 3.047863483428955\n",
      "\t Training Batch Loss: 3.1245110034942627\n",
      "\t Training Batch Loss: 2.7520079612731934\n",
      "\t Training Batch Loss: 2.9858837127685547\n",
      "\t Training Batch Loss: 2.7970645427703857\n",
      "\t Training Batch Loss: 2.93359637260437\n",
      "\t Training Batch Loss: 3.0071299076080322\n",
      "\t Training Batch Loss: 2.790524959564209\n",
      "\t Training Batch Loss: 2.8212568759918213\n",
      "\t Training Batch Loss: 2.673612594604492\n",
      "\t Training Batch Loss: 2.80847430229187\n",
      "\t Training Batch Loss: 2.6056008338928223\n",
      "\t Training Batch Loss: 2.712062120437622\n",
      "\t Training Batch Loss: 2.755770683288574\n",
      "\t Training Batch Loss: 2.592075824737549\n",
      "\t Training Batch Loss: 2.6419599056243896\n",
      "\t Training Batch Loss: 2.61076283454895\n",
      "\t Training Batch Loss: 2.557363748550415\n",
      "\t Training Batch Loss: 2.6379408836364746\n",
      "\t Training Batch Loss: 2.6722326278686523\n",
      "\t Training Batch Loss: 2.595794916152954\n",
      "\t Training Batch Loss: 2.6262779235839844\n",
      "\t Training Batch Loss: 2.5650548934936523\n",
      "\t Training Batch Loss: 2.71114182472229\n",
      "\t Training Batch Loss: 2.908559560775757\n",
      "\t Training Batch Loss: 3.3432211875915527\n",
      "\t Training Batch Loss: 2.7600579261779785\n",
      "\t Training Batch Loss: 2.9944305419921875\n",
      "\t Training Batch Loss: 2.989478588104248\n",
      "\t Training Batch Loss: 3.158160924911499\n",
      "\t Training Batch Loss: 2.9906654357910156\n",
      "\t Training Batch Loss: 2.8774116039276123\n",
      "\t Training Batch Loss: 3.0835609436035156\n",
      "\t Training Batch Loss: 2.920253276824951\n",
      "\t Training Batch Loss: 2.826827049255371\n",
      "\t Training Batch Loss: 2.732945680618286\n",
      "\t Training Batch Loss: 3.1107466220855713\n",
      "\t Training Batch Loss: 3.794726610183716\n",
      "\t Training Batch Loss: 3.052408456802368\n",
      "\t Training Batch Loss: 3.4763593673706055\n",
      "\t Training Batch Loss: 3.328221559524536\n",
      "\t Training Batch Loss: 2.9151265621185303\n",
      "\t Training Batch Loss: 3.4144275188446045\n",
      "\t Training Batch Loss: 3.275838613510132\n",
      "\t Training Batch Loss: 3.1491243839263916\n",
      "\t Training Batch Loss: 3.212489604949951\n",
      "\t Training Batch Loss: 3.116889476776123\n",
      "\t Training Batch Loss: 2.9755280017852783\n",
      "\t Training Batch Loss: 2.9091854095458984\n",
      "\t Training Batch Loss: 2.988626480102539\n",
      "\t Training Batch Loss: 2.753635883331299\n",
      "\t Training Batch Loss: 3.0219032764434814\n",
      "\t Training Batch Loss: 2.8696601390838623\n",
      "\t Training Batch Loss: 2.7542104721069336\n",
      "\t Training Batch Loss: 2.8390462398529053\n",
      "\t Training Batch Loss: 2.7247722148895264\n",
      "\t Training Batch Loss: 2.7379961013793945\n",
      "\t Training Batch Loss: 2.6742007732391357\n",
      "\t Training Batch Loss: 2.72501277923584\n",
      "\t Training Batch Loss: 2.5932042598724365\n",
      "\t Training Batch Loss: 2.619967222213745\n",
      "\t Training Batch Loss: 2.547729969024658\n",
      "\t Training Batch Loss: 2.6981682777404785\n",
      "\t Training Batch Loss: 2.5512566566467285\n",
      "\t Training Batch Loss: 2.5938572883605957\n",
      "\t Training Batch Loss: 2.595791816711426\n",
      "\t Training Batch Loss: 2.662708044052124\n",
      "\t Training Batch Loss: 2.4649109840393066\n",
      "\t Training Batch Loss: 2.610365867614746\n",
      "\t Training Batch Loss: 2.5527126789093018\n",
      "\t Training Batch Loss: 2.464232921600342\n",
      "\t Training Batch Loss: 2.5659706592559814\n",
      "\t Training Batch Loss: 2.5814852714538574\n",
      "\t Training Batch Loss: 2.446530342102051\n",
      "\t Training Batch Loss: 2.419255495071411\n",
      "\t Training Batch Loss: 2.5077145099639893\n",
      "\t Training Batch Loss: 2.422382354736328\n",
      "\t Training Batch Loss: 2.407897472381592\n",
      "\t Training Batch Loss: 2.522228717803955\n",
      "\t Training Batch Loss: 2.3935632705688477\n",
      "\t Training Batch Loss: 2.414462089538574\n",
      "\t Training Batch Loss: 2.4020984172821045\n",
      "\t Training Batch Loss: 2.520642042160034\n",
      "\t Training Batch Loss: 2.472886800765991\n",
      "\t Training Batch Loss: 2.5573389530181885\n",
      "\t Training Batch Loss: 2.5579123497009277\n",
      "\t Training Batch Loss: 2.7112138271331787\n",
      "\t Training Batch Loss: 3.0046629905700684\n",
      "\t Training Batch Loss: 3.4393045902252197\n",
      "\t Training Batch Loss: 2.72415828704834\n",
      "\t Training Batch Loss: 3.2510101795196533\n",
      "\t Training Batch Loss: 2.8233001232147217\n",
      "\t Training Batch Loss: 3.1114964485168457\n",
      "\t Training Batch Loss: 3.130265474319458\n",
      "\t Training Batch Loss: 2.8461148738861084\n",
      "\t Training Batch Loss: 3.05383563041687\n",
      "\t Training Batch Loss: 2.951894760131836\n",
      "\t Training Batch Loss: 2.9340639114379883\n",
      "\t Training Batch Loss: 2.9104747772216797\n",
      "\t Training Batch Loss: 2.83274507522583\n",
      "\t Training Batch Loss: 3.0510153770446777\n",
      "\t Training Batch Loss: 2.821460247039795\n",
      "\t Training Batch Loss: 2.874722719192505\n",
      "\t Training Batch Loss: 2.8063786029815674\n",
      "\t Training Batch Loss: 2.8248536586761475\n",
      "\t Training Batch Loss: 2.810577630996704\n",
      "\t Training Batch Loss: 2.7467498779296875\n",
      "\t Training Batch Loss: 2.8266186714172363\n",
      "\t Training Batch Loss: 2.7843594551086426\n",
      "\t Training Batch Loss: 2.7912745475769043\n",
      "\t Training Batch Loss: 2.7233355045318604\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset\n",
    "dataset_train = ChessIterableDataset(csv_files_train)\n",
    "dataset_val = ChessIterableDataset(csv_files_val)\n",
    "\n",
    "# Create a data loader\n",
    "train_data_loader = DataLoader(dataset_train, \n",
    "                               batch_size = 50000,\n",
    "                            #    shuffle=True, # include in version 1-3\n",
    ")\n",
    "\n",
    "\n",
    "val_data_loader = DataLoader(dataset_val, \n",
    "                             batch_size = 50000,\n",
    "                            #  shuffle=True, # include in version 1-3\n",
    ")\n",
    "\n",
    "\n",
    "model = EvalNet()\n",
    "model = torch.load('models_autosave/autosave2-0.pth')\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.L1Loss() # nn.MSELoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "train_loss_hist, valid_loss_hist = train(model, train_data_loader, val_data_loader, criterion, optimizer, num_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f'models_5_3/model{MODEL_NUMBER}-{MODEL_VERSION}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('pickle/DL_2_3-3_train_loss_history.pkl', 'wb') as f:\n",
    "    pickle.dump(train_loss_hist, f)\n",
    "\n",
    "with open('pickle/DL_2_3-3_valid_loss_history.pkl', 'wb') as f:\n",
    "    pickle.dump(valid_loss_hist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_loss_hist, label='Training Loss')\n",
    "plt.plot(valid_loss_hist, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
